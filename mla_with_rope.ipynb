{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Latent Attention with Rotary Positional Embeddings\n",
    "\n",
    "### How does MLA with RoPE work?\n",
    "\n",
    "Let's break down the process step by step:\n",
    "\n",
    "- **Input:**  \n",
    "  X (shape: [2, 768])\n",
    "\n",
    "- **Query projection:**  \n",
    "  Q = X * Wq (shape: [2, 768])\n",
    "\n",
    "- **Latent key-value space:**  \n",
    "  Ckv = X * Wdkv (shape: [2, 512])\n",
    "\n",
    "- **Key and Value projections:**  \n",
    "  K = Ckv * Wuk (shape: [2, 768])  \n",
    "  V = Ckv * Wuv (shape: [2, 768])\n",
    "\n",
    "- **Attention Scores:**  \n",
    "  Attention_Scores = Q * K_transpose (shape: [2, 2])\n",
    "\n",
    "#### Expanded view of attention computation:\n",
    "\n",
    "- Q * K_transpose = X * Wq * (Ckv * Wuk)_transpose\n",
    "- This can be rewritten as: X * Wq * Wuk_transpose * Ckv_transpose\n",
    "- Note: Wdkv * Wuk_transpose is a fixed matrix, so the expression becomes:  \n",
    "  X * (Wq * Wuk_transpose) * (X * Wdkv)_transpose\n",
    "\n",
    "- The term X * (Wq * Wuk_transpose) is sometimes called the \"absorbed query\".\n",
    "- The term (X * Wdkv)_transpose (which is Ckv_transpose) can be cached for efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### Absorbed Query and Latent KV Caching\n",
    "\n",
    "- The \"absorbed query\" (X * (Wq * Wuk_transpose)) can be precomputed and stored.\n",
    "- For each new input x (single or multiple tokens), you only need to bring in the absorbed query and the cached Ckv (both are precalculated).\n",
    "- This allows you to calculate attention scores efficiently, saving memory on the key-value (KV) cache, since you are storing the latent key-value space instead.\n",
    "- As a result, both memory usage and computation are improved.\n",
    "\n",
    "---\n",
    "\n",
    "### What changes when introducing RoPE?\n",
    "\n",
    "- When you introduce RoPE (Rotary Positional Embeddings), you need to rotate the query first and then rotate the key before calculating the attention scores.\n",
    "- The computation becomes:\n",
    "\n",
    "  ```\n",
    "  Attention_Scores = Q * K_transpose = X * Wq * (Ckv * Wuk)_transpose\n",
    "  Attention_Scores = Rope(X * Wq) * Rope(Ckv * Wuk)_transpose\n",
    "  ```\n",
    "\n",
    "- By doing this, you lose the absorbed query optimization, because the rotation must be applied after the projections, and you also have to calculate the key and then apply the rotation.\n",
    "- This means you can't precompute and cache the absorbed query as before.\n",
    "\n",
    "---\n",
    "\n",
    "### DeepSeek's Approach\n",
    "\n",
    "- To address this limitation, DeepSeek splits the query and key into two parts:\n",
    "  - One part where RoPE is applied.\n",
    "  - One part where RoPE is not applied (and uses the same latent key-value cache as before).\n",
    "- For the part where RoPE is applied:\n",
    "  - Multi-Query Attention (MQA) is used, meaning the head weights are shared across all heads for the RoPE part.\n",
    "  - The keys for this part are directly cached for a single head, and since the weights are shared, this is sufficient for all heads.\n",
    "- Both of the parts will give their own attention scores and we will add them together to get the final attention scores.\n",
    "- And once we have the added attention scores we can get the value matrix which remains the same as before; it is still an up-projection from the latent key-value space and hence obtain the context vector.\n",
    "- For the query, both down and up projections are performed. (NOT SURE WHAT THIS MEANS BUT it was mentioned about reducing activation memory during training.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def apply_rope(x: torch.Tensor, pos: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    RoPE-rotate q or k.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    x   : (B, H, S, dh)  – even dh required\n",
    "    pos : (S,)  or  (B, S)  – absolute token indices\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_rot : same shape as x, with RoPE applied\n",
    "    \"\"\"\n",
    "\n",
    "    dh = x.shape[-1]\n",
    "    assert dh % 2 == 0, \"dh must be even\"\n",
    "\n",
    "    half = dh // 2\n",
    "\n",
    "    device, dtype = x.device, x.dtype\n",
    "\n",
    "    idx = torch.arange(half, device=device, dtype=dtype)\n",
    "    freqs = 1.0 / (10000 ** (idx / half))\n",
    "\n",
    "    if pos.dim() == 1:                                   # (S,)  → (S,1)\n",
    "        theta = pos.to(dtype).unsqueeze(-1) * freqs          # (S, half)\n",
    "        # bring to  (1, 1, S, half)  so it lines up with (B,H,S,dh)\n",
    "        theta = theta.unsqueeze(0).unsqueeze(0)\n",
    "    else:\n",
    "        theta = pos.to(dtype).unsqueeze(-1) * freqs # (B, S, half)\n",
    "        theta = theta.unsqueeze(1) # (B, 1, S, half)\n",
    "\n",
    "    sin, cos = theta.sin(), theta.cos()\n",
    "\n",
    "    x_first_half = x[..., :half]\n",
    "    x_second_half = x[..., half:]\n",
    "\n",
    "    x_first_half_rot = x_first_half * cos - x_second_half * sin\n",
    "    x_second_half_rot = x_first_half * sin + x_second_half * cos\n",
    "\n",
    "    x_rot = torch.cat([x_first_half_rot, x_second_half_rot], dim=-1)\n",
    "\n",
    "    return x_rot\n",
    "\n",
    "\n",
    "class MLAWithRope(nn.Module):\n",
    "    def __init__(self, d_model,n_heads,n_heads_rope,kv_latent_dim):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_heads_rope = n_heads_rope\n",
    "        self.kv_latent_dim = kv_latent_dim\n",
    "        self.dh = d_model // n_heads\n",
    "        self.dh_rope = d_model // n_heads_rope\n",
    "\n",
    "        # Weight for MLA without Rope\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_dkv = nn.Linear(d_model, kv_latent_dim, bias=False)\n",
    "        self.W_uk = nn.Linear(kv_latent_dim, d_model, bias=False)\n",
    "        self.W_uv = nn.Linear(kv_latent_dim, d_model, bias=False)\n",
    "\n",
    "        # Weight for MLA with Rope\n",
    "        self.W_q_rope = nn.Linear(d_model, self.dh_rope, bias=False)\n",
    "        self.W_k_rope = nn.Linear(d_model,self.dh_rope, bias=False) # becuase all heads share the same weight matrix in case of Rope\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.ln1 = nn.LayerNorm(kv_latent_dim)\n",
    "\n",
    "        self.register_buffer('absorbed_k', None)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, latent_kv_cache: Optional[torch.Tensor] = None, rope_key_cache: Optional[torch.Tensor] = None, past_length: int = 0) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        B, S, D = x.shape # batch size, sequence length, d_model\n",
    "\n",
    "        assert D == self.n_heads * self.dh, \"d_model must be divisible by n_heads\"\n",
    "        assert D == self.n_heads_rope * self.dh_rope, \"d_model must be divisible by n_head_rope\"\n",
    "\n",
    "        if self.absorbed_k is None:\n",
    "            absorbed = torch.matmul(self.W_q.weight,self.W_uk.weight) # the transpose is done automatically in matmul (d_model, kv_latent_dim)\n",
    "            self.absorbed_k = absorbed.view(self.n_heads,self.dh, -1) # (n_heads, dh, kv_latent_dim)\n",
    "        \n",
    "        # ckv basically means the latent key value space\n",
    "        new_c_kv = self.ln1(self.W_dkv(x)) # (B, S, kv_latent_dim)\n",
    "        if latent_kv_cache is None:\n",
    "            latent_kv_cache = new_c_kv\n",
    "        else:\n",
    "            latent_kv_cache = torch.cat([latent_kv_cache, new_c_kv], dim=1) # (B, S_full, kv_latent_dim)\n",
    "\n",
    "        assert latent_kv_cache is not None, \"latent_kv_cache should never be None here\"\n",
    "\n",
    "        s_full = latent_kv_cache.size(1)\n",
    "\n",
    "        # MLA without Rope\n",
    "\n",
    "        x_per_head = x.view(B, S, self.n_heads, self.dh) # (B, S, n_heads, dh)\n",
    "        x_per_head = x_per_head.transpose(1, 2) # (B, n_heads, S, dh)\n",
    "\n",
    "        # (B, n_heads, S, dh) * (n_heads, dh, kv_latent_dim) -> (B, n_heads, S, kv_latent_dim)\n",
    "        q_lat = torch.matmul(x_per_head, self.absorbed_k) # (B, n_heads, S, d_model)\n",
    "\n",
    "        # (B, n_heads, S, kv_latent_dim) * (B, n_heads, s_full, kv_latent_dim).transpose(1, 2) -> (B, n_heads, S, s_full)\n",
    "        scores = torch.matmul(q_lat, latent_kv_cache.transpose(1, 2)) # (B, n_heads, S, s_full)\n",
    "\n",
    "        # MLA with Rope\n",
    "        # here all heads share the same weight matrix, \n",
    "\n",
    "        # (B, S, D) -> (B, S, dh_rope)\n",
    "        q_rope_per_head: torch.Tensor= self.W_q_rope(x).unsqueeze(1) # (B, 1, S, dh_rope)\n",
    "        k_rope_per_head: torch.Tensor = self.W_k_rope(x).unsqueeze(1) # (B, 1, S, dh_rope)\n",
    "\n",
    "        pos_cur = torch.arange(past_length, past_length + S, device=x.device)\n",
    "        q_rope_per_head = apply_rope(q_rope_per_head, pos_cur) # (B, 1, S, dh_rope)\n",
    "        k_rope_per_head = apply_rope(k_rope_per_head, pos_cur) # (B, 1, S, dh_rope)\n",
    "\n",
    "\n",
    "        if rope_key_cache is None:\n",
    "            rope_key_cache = k_rope_per_head\n",
    "        else:\n",
    "            rope_key_cache = torch.cat([rope_key_cache, k_rope_per_head], dim=2) # (B, 1, S_full, dh_rope)\n",
    "\n",
    "        q_rope = q_rope_per_head.expand(-1, self.n_heads_rope, -1, -1) # (B, n_heads_rope, S, dh_rope)\n",
    "        k_rope = rope_key_cache.expand(-1, self.n_heads_rope, -1, -1) # (B, n_heads_rope, S_full, dh_rope))\n",
    "\n",
    "        # (B, n_heads, S, dh_rope) * (B, n_heads, S_full, dh_rope).transpose(1, 2) -> (B, n_heads_rope, S, S_full)\n",
    "        scores_rope = torch.matmul(q_rope, k_rope.transpose(2, 3)) # (B, n_heads_rope, S, S_full)\n",
    "\n",
    "        print(f\"final attention scores : {scores.shape} and {scores_rope.shape}\")\n",
    "\n",
    "\n",
    "        # final attention scores are addition of scores from MLA without Rope and MLA with Rope\n",
    "        final_attention_scores = scores + scores_rope # (B, n_heads_rope, S, S_full)\n",
    "\n",
    "\n",
    "        final_attention_scores: torch.Tensor = final_attention_scores / (self.dh ** 0.5)\n",
    "\n",
    "        mask = torch.tril(torch.ones(S,s_full,device=x.device),diagonal=past_length) # diagonal -> shifting the diagonal by past_length\n",
    "\n",
    "        final_attention_scores = final_attention_scores.masked_fill(mask.view(1,1,S,s_full) == 0, float('-inf')) # (B, n_heads_rope, S, S_full)\n",
    "\n",
    "        attention_weights = F.softmax(final_attention_scores,dim=-1) # (B, n_heads_rope,S,S_full)\n",
    "\n",
    "\n",
    "        # value matrix will be extracted from the latent key value space\n",
    "        value_matrix: torch.Tensor = self.W_uv(latent_kv_cache) # (B, S_full, d_model)\n",
    "        value_matrix = value_matrix.view(B,s_full,self.n_heads_rope,self.dh_rope).transpose(1,2) # (B, n_heads_rope, S_full, dh_rope)\n",
    "\n",
    "        context_matrix = torch.matmul(attention_weights,value_matrix) # (B,n_heads_rope,S,dh_rope)\n",
    "        \n",
    "        context_matrix = context_matrix.transpose(1, 2).contiguous().view(B, S, self.d_model)\n",
    "\n",
    "        return self.W_o(context_matrix), latent_kv_cache, rope_key_cache        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final attention scores : torch.Size([1, 2, 5, 5]) and torch.Size([1, 2, 5, 5])\n",
      "Step 1:\n",
      "Output shape: torch.Size([1, 5, 8])\n",
      "KV shape: torch.Size([1, 5, 4])\n",
      "Rope Key cache shape: torch.Size([1, 1, 5, 4])\n",
      "final attention scores : torch.Size([1, 2, 1, 6]) and torch.Size([1, 2, 1, 6])\n",
      "Step 2:\n",
      "Output shape: torch.Size([1, 1, 8])\n",
      "KV shape: torch.Size([1, 6, 4])\n",
      "Rope Key cache shape: torch.Size([1, 1, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "def demo_mla():\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    model = MLAWithRope(d_model=8,n_heads=2,n_heads_rope=2,kv_latent_dim=4)\n",
    "\n",
    "    ## we have to keep the n_heads and n_heads_rope same becaue then only we will be able to concatenate the attention scores from both the parts\n",
    "    \n",
    "    x1 = torch.randn(1,5,8) # (batch_size, seq_len, d_model)\n",
    "    out1, latent_kv1, k_rope = model(x1)\n",
    "    \n",
    "    print(\"Step 1:\")\n",
    "    print(f\"Output shape: {out1.shape}\")\n",
    "    print(f\"KV shape: {latent_kv1.shape}\")\n",
    "    print(f\"Rope Key cache shape: {k_rope.shape}\")\n",
    "    \n",
    "    x2 = torch.randn(1,1,8) # (batch_size, seq_len, d_model)\n",
    "    out2, latent_kv2, k_rope2 = model(x2,latent_kv_cache=latent_kv1,rope_key_cache=k_rope,past_length=5)\n",
    "    \n",
    "    print(\"Step 2:\")\n",
    "    print(f\"Output shape: {out2.shape}\")\n",
    "    print(f\"KV shape: {latent_kv2.shape}\")\n",
    "    print(f\"Rope Key cache shape: {k_rope2.shape}\")\n",
    "    \n",
    "demo_mla()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
