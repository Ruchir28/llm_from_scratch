{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x111a2c390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKRouter(nn.Module):\n",
    "    def __init__(self, n_embd, n_experts, top_k):\n",
    "        super().__init__()\n",
    "        self.n_embd = n_embd\n",
    "        self.n_experts = n_experts\n",
    "        self.top_k = top_k\n",
    "        self.linear = nn.Linear(n_embd, n_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, self.top_k, dim=-1)\n",
    "        negative_inf = torch.full_like(logits, float(\"-inf\"))\n",
    "        sparse_logits = negative_inf.scatter(dim=-1, index=top_k_indices, src=top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5747, 0.4253, 0.0000],\n",
      "         [0.3194, 0.0000, 0.6806],\n",
      "         [0.3203, 0.0000, 0.6797],\n",
      "         [0.5498, 0.4502, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0, 1],\n",
      "         [2, 0],\n",
      "         [2, 0],\n",
      "         [0, 1]]])\n"
     ]
    }
   ],
   "source": [
    "num_experts = 3 \n",
    "top_k = 2\n",
    "n_embd = 8\n",
    "\n",
    "mh_output = torch.randn(1,4,n_embd)\n",
    "top_k_router = TopKRouter(n_embd, num_experts, top_k)\n",
    "router_output, top_k_indices = top_k_router(mh_output)\n",
    "print(router_output)\n",
    "print(top_k_indices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoE(torch.nn.Module):\n",
    "    def __init__(self, n_embd, n_experts, top_k, dropout):\n",
    "        super().__init__()\n",
    "        self.router = TopKRouter(n_embd, n_experts, top_k)\n",
    "        self.experts = nn.ModuleList([Expert(n_embd, dropout) for _ in range(n_experts)])\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gating_output, top_k_indices = self.router(x)\n",
    "        final_output = torch.zeros_like(x) # (batch_size, seq_len, n_embd)\n",
    "        # gating output -> (batch_size, seq_len, n_experts)\n",
    "        # top_k_indices -> (batch_size, seq_len, top_k)\n",
    "        flat_x = x.view(-1,x.size(-1)) # (batch_size * seq_len, n_embd)\n",
    "        flat_gating_output = gating_output.view(-1,gating_output.size(-1)) # (batch_size * seq_len, n_experts)\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = (top_k_indices == i).any(dim=-1) # (batch_size,seq_len)\n",
    "            flat_mask = mask.view(-1) # (batch_size * seq_len)\n",
    "            # let current_expert_selected be the number of tokens selected for current expert\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_x[flat_mask] # (current_expert_selected, n_embd)\n",
    "                expert_output = expert(expert_input) # (current_expert_selected, n_embd)\n",
    "\n",
    "                gating_scores = flat_gating_output[flat_mask,i].unsqueeze(-1) # (current_expert_selected,1)\n",
    "                weighted_expert_output = expert_output * gating_scores # (current_expert_selected, n_embd)\n",
    "\n",
    "                final_output[mask] = weighted_expert_output\n",
    "        \n",
    "        return final_output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 4, 8])\n",
      "tensor([[[ 0.0000e+00, -2.0952e-01,  1.1119e-01, -3.4127e-01,  5.1096e-02,\n",
      "           0.0000e+00,  1.0927e-01,  1.1165e-01],\n",
      "         [ 0.0000e+00, -2.2709e-04,  2.1775e-01, -1.5330e-01,  6.7753e-02,\n",
      "          -2.0473e-01, -2.4894e-01,  0.0000e+00],\n",
      "         [ 1.3526e-01,  0.0000e+00,  7.6696e-01,  1.0333e-01,  6.1043e-02,\n",
      "          -0.0000e+00, -0.0000e+00,  3.0101e-01],\n",
      "         [ 3.8247e-02, -2.0839e-01,  3.1646e-01, -0.0000e+00,  2.6404e-02,\n",
      "           0.0000e+00,  1.2235e-01,  1.0671e-01]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)\n",
    "\n",
    "num_experts = 3 \n",
    "top_k = 2\n",
    "n_embd = 8\n",
    "dropout = 0.1\n",
    "\n",
    "mh_output = torch.randn(1,4,n_embd)\n",
    "sparse_moe = SparseMoE(n_embd, num_experts, top_k, dropout)\n",
    "output = sparse_moe(mh_output)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
