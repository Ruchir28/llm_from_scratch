{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"Hello, world. This is a Test-- ?.\"\n",
    "result = re.split(r'([,.:;?_!()\\']|--|\\s)',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', 'is', 'a', 'Test', '--', '?', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "## Tokenization\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1132\n"
     ]
    }
   ],
   "source": [
    "## create a list of all tokens and sort them to determine the vocabulary size\n",
    "tokens = sorted(set(preprocessed))\n",
    "all_tokens = tokens.extend(['<|endoftext|>','<|unk|>'])\n",
    "vocab_size = len(tokens)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!: 0\n",
      "\": 1\n",
      "': 2\n",
      "(: 3\n",
      "): 4\n",
      ",: 5\n",
      "--: 6\n",
      ".: 7\n",
      ":: 8\n",
      ";: 9\n"
     ]
    }
   ],
   "source": [
    "## now creating vocab i.e. a dictionary that maps each token to a unique integer\n",
    "vocab = {token:integer for integer,token in enumerate(tokens)}\n",
    "\n",
    "## print the first 10 tokens and their corresponding ids\n",
    "for token, integer in list(vocab.items())[:10]:\n",
    "    print(f\"{token}: {integer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {integer:token for token,integer in vocab.items()}\n",
    "    \n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        return [self.str_to_int[token] for token in preprocessed]\n",
    "    \n",
    "    def decode(self,tokens):\n",
    "        text = \" \".join([self.int_to_str[token] for token in tokens])\n",
    "        ## replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\']|--)', r'\\1', text)\n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "## encode a sample text\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "\n",
    "## decode the ids back to text\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fails to tokenize word not in vocab\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {integer:token for token,integer in vocab.items()}\n",
    "    \n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [token if token in self.str_to_int else '<|unk|>' for token in preprocessed]\n",
    "        return [self.str_to_int[token] for token in preprocessed]\n",
    "    \n",
    "    def decode(self,tokens):\n",
    "        text = \" \".join([self.int_to_str[token] for token in tokens])\n",
    "        ## replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\']|--)', r'\\1', text)\n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1131, 5, 1131, 7, 97, 584, 115, 1131, 6, 10, 7]\n",
      "\" <|unk|>, <|unk|>. This is a <|unk|>--?.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "## encode a sample text\n",
    "text = \"\"\"\"Hello, world. This is a Test-- ?.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "\n",
    "## decode the ids back to text\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world. This is a Test-- ?. <|endoftext|> In the sunlit meadows, the flowers bloom.\n",
      "[1131, 5, 1131, 7, 97, 584, 115, 1131, 6, 10, 7, 1130, 55, 988, 956, 1131, 5, 988, 449, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, world. This is a Test-- ?.\"\n",
    "text2 = \"In the sunlit meadows, the flowers bloom.\"\n",
    "\n",
    "## when we have 2 text source, we join them with a special token\n",
    "text = text1 + \" <|endoftext|> \" + text2\n",
    "\n",
    "print(text)\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, <|unk|>. This is a <|unk|>--?. <|endoftext|> In the sunlit <|unk|>, the flowers <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
