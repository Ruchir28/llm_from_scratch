{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![transformer_block](transformer_block.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.attention import MultiHeadAttention\n",
    "from utils.layers import FeedForward, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=config[\"emb_dim\"],\n",
    "            d_out=config[\"emb_dim\"],\n",
    "            context_length=config[\"context_length\"],\n",
    "            dropout=config[\"drop_rate\"],\n",
    "            num_heads=config[\"n_heads\"],\n",
    "            qkv_bias=config[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(config)\n",
    "        self.norm1 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.drop_skip_layer = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "    def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_skip_layer(x)\n",
    "        x = x + shortcut\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_skip_layer(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0620,  0.5158, -0.0550,  ...,  1.2767,  0.1946,  0.6363],\n",
       "         [-0.0909,  0.1276,  0.2290,  ...,  0.6513,  0.5151,  0.7462],\n",
       "         [ 0.4857,  0.4642, -0.0245,  ...,  1.2237,  0.1653,  0.7574],\n",
       "         [ 0.0807,  0.8251,  0.8783,  ...,  0.4391,  0.6565,  0.7821]],\n",
       "\n",
       "        [[ 0.3090,  1.2133,  0.5646,  ...,  0.2144, -0.0115, -0.4829],\n",
       "         [-0.1076,  0.7464,  0.2806,  ...,  0.2268,  0.5715,  0.0618],\n",
       "         [ 0.8299,  0.6553,  0.3559,  ...,  0.4327,  0.7580, -0.0314],\n",
       "         [ 0.5128,  0.6655,  0.1042,  ...,  1.2630,  1.3332,  0.2348]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "\n",
    "transformer_block = TransformerBlock(GPT_CONFIG_124M)\n",
    "transformer_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
