{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "- Layer normalisation is helful as it prevent problems such as vainishing gradients, and makes the training efficient  \n",
    "- When we use layer normalisation,the mean and std deviation of output from each layer is fixed and hence this is helfpul in having a efficeint training and prevents internal covariate shift \n",
    "- What exactly is internal covariate shift? \n",
    "     - Internal covariate shift is when the distribution of the input changes with the layer. \n",
    "     - This is problematic as it causes the network to have to readjust the weights to compensate for the change in distribution. \n",
    "     - This slows down the training process and makes it less efficient. \n",
    "     - Layer normalisation helps to prevent this by standardising the input to each layer. \n",
    "     - This makes the training process more efficient and faster. \n",
    "     - It also helps to prevent the problem of vanishing gradients. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class LayerNorm(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,emd_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = torch.ones(emd_dim)\n",
    "        self.shift = torch.zeros(emd_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1,keepdim=True)\n",
    "        std = x.std(dim=-1,keepdim=True,unbiased=False) # unbiased=False, it uses division by N instead of N-1\n",
    "        return self.scale * (x - mean) / (std + self.eps) + self.shift\n",
    "    \n",
    "    ## The scale and shift are learnable parameters that are used to scale and shift the input \n",
    "    # to the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example  = torch.randn(2,5)\n",
    "layer = torch.nn.Sequential(torch.nn.Linear(5,6),torch.nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]])\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(5)\n",
    "out = ln(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([[-2.9802e-08],\n",
      "        [ 0.0000e+00]])\n",
      " std: tensor([[1.0000],\n",
      "        [1.0000]])\n"
     ]
    }
   ],
   "source": [
    "## verify the output\n",
    "mean = out.mean(dim=-1,keepdim=True)\n",
    "std = out.std(dim=-1,keepdim=True,unbiased=False)\n",
    "\n",
    "print(f\"mean: {mean}\\n std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
