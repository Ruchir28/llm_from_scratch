{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not use word-level tokenization?\n",
    "\n",
    "There may be rare words that don't occur in the training data and hence will be out of vocabulary.\n",
    "\n",
    "The tokens such as boy and boys are related but will be tokenized as two different tokens.\n",
    "\n",
    "### Byte-level tokenization\n",
    "\n",
    "This is a type of subword tokenization.\n",
    "\n",
    "It is a type of character-level tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.0\n"
     ]
    }
   ],
   "source": [
    "tiktoken_version = importlib.metadata.version('tiktoken')\n",
    "print(tiktoken_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 995, 0, 220, 50256, 770, 318, 257, 1332, 13]\n",
      "Hello, world! <|endoftext|> This is a test.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = (\n",
    "    \"Hello, world! <|endoftext|> This is a test.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "decoded_text = tokenizer.decode(integers)\n",
    "\n",
    "print(integers)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2954, 77, 26502, 4775]\n",
      "[b'unk', b'n', b'won', b'word']\n",
      "unknwonword\n"
     ]
    }
   ],
   "source": [
    "## check how this takes care of unknown tokens\n",
    "\n",
    "text = \"unknwonword\"\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "decoded_text = tokenizer.decode(integers)\n",
    "\n",
    "decoded_array = tokenizer.decode_tokens_bytes(integers)\n",
    "\n",
    "print(integers)\n",
    "print(decoded_array) ## to visualise how this unknown token is handled\n",
    "print(decoded_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
