{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Why Multi-Head Latent Attention?\n",
    "\n",
    "So the main reason for using multi-head latent attention is to reduce the size required to cache the key and value vectors\n",
    "or we say the KV cache size\n",
    "\n",
    "### Why is KV cache size important to reduce ?\n",
    "\n",
    "The KV cache size is important to reduce because it directly affects the memory usage of the model. A larger KV cache size means more memory is required to store the key and value vectors, which can lead to higher memory costs and potential out-of-memory errors. By reducing the KV cache size, we can optimize memory usage and improve the overall efficiency of the model.\n",
    "\n",
    "#### Why to even store them ?\n",
    "\n",
    "The key and value vectors are stored in the cache to avoid redundant calculations during the attention process. When the same input is processed multiple times, the key and value vectors can be reused, which can improve performance and reduce memory usage.\n",
    "\n",
    "#### How does mutli-head latent attention help in reducing the KV cache size ?\n",
    "\n",
    "In mutli-head latent attention, we use latent variables to represent the key and value vectors, which can be computed once and reused for multiple tokens. This reduces the size of the KV cache required to store the key and value vectors for each token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it plays out ?\n",
    "\n",
    "Let's say our input matrix is of shape [2,768] i.e. 2 tokens and 768 dimensions (embeddings)\n",
    "\n",
    "Now generally what we would have done is convert it to K,Q,V vectors of shape [2,768] each\n",
    "And for cache we would have stored the key and value vectors \n",
    "\n",
    "But in case of latent attention what is done is :\n",
    "1. query is calculated the same way \n",
    "2. but now input is first converted to a latent key value space (which signifies compressed key value data)\n",
    "    Wdkv -> Weight for down projection key and value [768,512]\n",
    "    i.e. [2.768] * Wdkv -> [2,512]\n",
    "    and then this latent key value space is used to calculate key and value\n",
    "    by doing up projection\n",
    "    Wuv -> Weight for up projection value [512,768]\n",
    "    i.e. [2,512] * Wuv -> [2,768]\n",
    "    Wuk -> Weight for up projection key [512,768]\n",
    "    i.e. [2,512] * Wuk -> [2,768]\n",
    "    So we obtained both key and value vectors\n",
    "    Now the number of steps muight seems to be increased (which is not the case as we can combine some calulcations) but we don't \n",
    "    need to store key and value vectors in cache seperately we can just store this latent key value space\n",
    "3. Let's see how the calculation of attention scores would be different\n",
    "    X -> input -> [2,768] \n",
    "    Q -> X * Wq -> [2,768]\n",
    "    Ckv -> latent key value space -> [2,512] -> X * Wdkv -> [2,512]\n",
    "    K -> Ckv * Wuk -> [2,768]\n",
    "    V -> Ckv * Wuv -> [2,768]\n",
    "    Attention scores -> Q * K.T -> [2,2] \n",
    "    Q * K.T -> X * Wdkv * (Ckv * Wuk).T \n",
    "    Q * K.T -> X * Wdkv * Wuk.T * Ckv.T (we can say the multiplication of weight i.e. Wdkv * Wuk.T) is something which is always fixed \n",
    "    Q * K.T -> X * (Wdkv * Wuk.T) * (X * Wdkv).T\n",
    "    X * (Wdkv * Wuk.T) -> Absorbed query \n",
    "    (X * Wdkv).T -> this is cached every time basically Ckv\n",
    "    \n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLA(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, kv_latent_dim):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.dh = d_model // n_heads # dimension per head\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_dkv = nn.Linear(d_model, kv_latent_dim, bias=False)\n",
    "        self.W_uk = nn.Linear(kv_latent_dim, d_model, bias=False)\n",
    "        self.W_uv = nn.Linear(kv_latent_dim, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False) # final output projection\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(kv_latent_dim)\n",
    "        self.register_buffer('absorbed_k', None) # as we figured out this stored (W_dkv @ W_uk)\n",
    "\n",
    "    def forward(self, x, kv_cache=None, past_length=0):\n",
    "        B,S,D = x.size() # batch, context length, dimension\n",
    "\n",
    "        if self.absorbed_k is None:\n",
    "            absorbed = torch.matmul(self.W_q.weight,self.W_uk.weight) # the transpose is done automatically in matmul (d_model, kv_latent_dim)\n",
    "            self.absorbed_k = absorbed.view(self.n_heads,self.dh, -1) # (n_heads, dh, kv_latent_dim)\n",
    "        \n",
    "        new_ckv = self.ln1(self.W_dkv(x)) # new latent kv cache for current input x \n",
    "        if kv_cache is None:\n",
    "            kv_cache = new_ckv\n",
    "        else:\n",
    "            kv_cache = torch.cat([kv_cache, new_ckv], dim=1) # (B, S, D)\n",
    "        \n",
    "        s_full = kv_cache.size(1)\n",
    "\n",
    "        v_full = self.W_uv(kv_cache) # (B, S, D)\n",
    "        v = v_full.view(B,s_full,self.n_heads,self.dh).transpose(1,2) # (B, n_heads, S_full, dh)\n",
    "\n",
    "        q = x.view(B,S,self.n_heads,self.dh)\n",
    "\n",
    "        attn_scores = torch.zeros(B,self.n_heads,S,s_full,device=x.device)\n",
    "\n",
    "        # why s and s_full?\n",
    "        # s is the current context length\n",
    "        # s_full is the total context length including the past context and for current prediction we need scores b/w current words with all previpous \n",
    "        # words including the current words i.e. [s, s_full] s_full = s + prev_context_length\n",
    "        # why not s_full,s_full because for next token prediction we just need the logits of last current word\n",
    "\n",
    "        for h in range(self.n_heads):\n",
    "            tmp = torch.matmul(q[:,:,h],self.absorbed_k[h])\n",
    "            attn_scores[:,h] = torch.bmm(tmp,kv_cache.transpose(1,2))\n",
    "\n",
    "        attn_scores = attn_scores / (self.dh ** 0.5)\n",
    "\n",
    "        mask = torch.tril(torch.ones(S,s_full,device=x.device),diagonal=past_length) # diagonal -> shifting the diagonal by past_length or visualise it like \n",
    "        # extending the diagoal to right by past_length why ? because we start masking from past_length + 1\n",
    "        # Creating causal attention mask:\n",
    "        # mask = torch.tril(torch.ones(S,S_full,device=x.device),diagonal=past_length)\n",
    "        #\n",
    "        # This creates a mask matrix of shape [S x S_full] where:\n",
    "        # - S is current sequence length\n",
    "        # - S_full is total sequence length (current + past context)\n",
    "        # - past_length = S_full - S (length of past context)\n",
    "        #\n",
    "        # Example with S=4, S_full=7, past_length=3:\n",
    "        # 1 1 1 1 0 0 0  # First row: can attend to past context (first 3) and itself\n",
    "        # 1 1 1 1 1 0 0  # Second row: can attend to all previous tokens and itself\n",
    "        # 1 1 1 1 1 1 0  # Third row: same pattern\n",
    "        # 1 1 1 1 1 1 1  # Fourth row: can attend to everything up to itself\n",
    "        #\n",
    "        # The diagonal parameter shifts the main diagonal up by past_length,\n",
    "        # allowing each position to attend to all past context tokens plus\n",
    "        # the regular causal attention pattern for current sequence.\n",
    "        attn_scores = attn_scores.masked_fill(mask.view(1,1,S,s_full) == 0, float('-inf'))\n",
    "\n",
    "        attention_weights = F.softmax(attn_scores, dim=-1)   # (B, n_heads, S, S_full)\n",
    "\n",
    "        out_heads = []\n",
    "\n",
    "        for h in range(self.n_heads):\n",
    "            context_h = torch.matmul(attention_weights[:,h], v[:,h]) # (B,S, dh)\n",
    "            out_heads.append(context_h)\n",
    "        \n",
    "        out = torch.cat(out_heads, dim=-1) # (B,S,D)\n",
    "\n",
    "        return self.W_o(out), kv_cache\n",
    "                \n",
    "        \n",
    "                \n",
    "        \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "Output shape: torch.Size([1, 5, 8])\n",
      "KV shape: torch.Size([1, 5, 4])\n",
      "Step 2:\n",
      "Output shape: torch.Size([1, 1, 8])\n",
      "KV shape: torch.Size([1, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "def demo_mla():\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    model = MLA(d_model=8,n_heads=2,kv_latent_dim=4)\n",
    "\n",
    "    x1 = torch.randn(1,5,8) # (batch_size, seq_len, d_model)\n",
    "    out1, kv1 = model(x1)\n",
    "    \n",
    "    print(\"Step 1:\")\n",
    "    print(f\"Output shape: {out1.shape}\")\n",
    "    print(f\"KV shape: {kv1.shape}\")\n",
    "    \n",
    "    x2 = torch.randn(1,1,8) # (batch_size, seq_len, d_model)\n",
    "    out2, kv2 = model(x2,kv_cache=kv1,past_length=5)\n",
    "    \n",
    "    print(\"Step 2:\")\n",
    "    print(f\"Output shape: {out2.shape}\")\n",
    "    print(f\"KV shape: {kv2.shape}\")\n",
    "    \n",
    "demo_mla()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
